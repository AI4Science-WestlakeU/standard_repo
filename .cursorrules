# Standard Research Repository - Cursor AI Rules

## Project Overview
This is a standard research project template for deep learning and scientific computing projects. The codebase follows academic research best practices with modular design, configuration-driven experiments, and reproducibility focus.

## Core Principles

### 1. Code Quality & Safety
- **Always follow PEP8** style guidelines with 88-character line limit
- **Type hints required** for all function signatures and class methods
- **Docstrings mandatory** for all classes, functions, and modules using Google style
- **Error handling**: Use try-except blocks for file I/O, model loading, and external API calls
- **Input validation**: Validate function arguments, especially file paths and configurations
- **No hardcoded paths**: Use configuration files or environment variables
- **Security**: Never commit API keys, passwords, or sensitive data

### 2. Project Structure Compliance
Follow the established directory structure:
```
standard_repo_module/
├── data/          # Dataset classes and data loading
├── model/         # Model definitions and architectures  
├── train/         # Training scripts and configurations
├── inference/     # Evaluation and inference scripts
├── utils/         # Utility functions and helpers
├── configs/       # Configuration files (YAML)
├── tests/         # Unit tests and integration tests
└── __init__.py    # Package initialization
```

### 3. Naming Conventions
- **Files**: `snake_case.py` (e.g., `train_demo.py`, `model_demo.py`)
- **Classes**: `PascalCase` (e.g., `Net_demo`, `Advection`)
- **Functions/Variables**: `snake_case` (e.g., `load_dataset`, `train_batch_size`)
- **Constants**: `UPPER_CASE` (e.g., `EXP_PATH`, `COLOR_LIST`)
- **Experiments**: Use date format `YYYY-MM-DD` for experiment folders

### 4. Configuration Management
- **Use YAML configs** for all hyperparameters and settings
- **Config inheritance**: Support base configs with overrides
- **Argument parsing**: Use argparse with config file integration
- **Save configurations** with each experiment run for reproducibility
- **Environment-specific configs**: Separate dev/prod/test configurations

### 5. Research-Specific Requirements

#### Experiment Management
- **Unique experiment IDs**: Combine date + experiment name
- **Result organization**: Save to `results/YYYY-MM-DD/exp_name/`
- **Checkpoint management**: Regular model saving with epoch numbers
- **Logging**: Use Python logging module with file and console output
- **Metrics tracking**: Integrate TensorBoard or WandB for visualization

#### Reproducibility
- **Seed everything**: Set seeds for Python, NumPy, PyTorch, and CUDA
- **Version tracking**: Log package versions and git commit hashes
- **Data versioning**: Track dataset versions and preprocessing steps
- **Environment specs**: Maintain requirements.txt and environment.yml

#### Code Documentation
- **README.md**: Include installation, usage, and citation information
- **Reproducibility statement**: Document how to reproduce results
- **API documentation**: Generate docs for all public functions
- **Experiment logs**: Detailed logging of training progress and metrics

### 6. PyTorch/ML Specific Guidelines

#### Model Development
- **Device handling**: Support both CPU and GPU with proper device management
- **Memory management**: Use `torch.cuda.empty_cache()` appropriately
- **Model saving**: Save state_dict, not entire model objects
- **Gradient handling**: Proper gradient accumulation and clipping

#### Data Handling
- **DataLoader optimization**: Use appropriate num_workers and pin_memory
- **Data validation**: Check tensor shapes and data types
- **Preprocessing**: Document and version all data transformations
- **Memory efficiency**: Use generators for large datasets

#### Training Best Practices
- **Progress tracking**: Use tqdm for progress bars
- **Learning rate scheduling**: Implement and log LR changes
- **Early stopping**: Implement validation-based early stopping
- **Gradient monitoring**: Log gradient norms and detect vanishing/exploding gradients

### 7. AI Assistant Guidelines

#### When Writing Code
- **Start with imports**: Organize imports (standard, third-party, local)
- **Add comprehensive docstrings**: Include Args, Returns, Raises, Examples
- **Include type hints**: Use typing module for complex types
- **Add inline comments**: Explain non-obvious logic and calculations
- **Handle edge cases**: Consider and handle potential failure modes
- **Write defensive code**: Validate inputs and handle errors gracefully

#### When Modifying Existing Code
- **Maintain consistency**: Follow existing patterns and style
- **Preserve functionality**: Don't break existing interfaces
- **Update documentation**: Modify docstrings and comments accordingly
- **Consider backward compatibility**: Avoid breaking changes when possible

#### When Creating New Features
- **Follow module structure**: Place code in appropriate directories
- **Create corresponding tests**: Write unit tests for new functionality
- **Update configurations**: Add new parameters to config files
- **Document usage**: Provide examples and usage instructions

### 8. Common Patterns to Use

#### Configuration Loading
```python
def load_config(config_path: str) -> dict:
    """Load YAML configuration file with error handling."""
    try:
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        raise FileNotFoundError(f"Config file not found: {config_path}")
    except yaml.YAMLError as e:
        raise ValueError(f"Invalid YAML in {config_path}: {e}")
```

#### Experiment Setup
```python
def setup_experiment(args) -> tuple[str, logging.Logger]:
    """Set up experiment directory and logging."""
    exp_dir = f"{args.results_path}/{args.date_exp}/{args.exp_name}"
    os.makedirs(exp_dir, exist_ok=True)
    
    logger = setup_logging(exp_dir)
    save_config(args, exp_dir)
    set_seed(args.seed)
    
    return exp_dir, logger
```

#### Model Training Loop
```python
def train_epoch(model, dataloader, optimizer, criterion, device):
    """Train model for one epoch with proper error handling."""
    model.train()
    total_loss = 0.0
    
    for batch_idx, (data, target) in enumerate(dataloader):
        try:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        except RuntimeError as e:
            logger.error(f"Training error at batch {batch_idx}: {e}")
            torch.cuda.empty_cache()
            continue
    
    return total_loss / len(dataloader)
```

### 9. What to Avoid
- **Global variables**: Use configuration objects instead
- **Hardcoded values**: Make everything configurable
- **Silent failures**: Always log errors and exceptions
- **Memory leaks**: Properly manage GPU memory
- **Inconsistent naming**: Follow established conventions
- **Missing documentation**: Document all public interfaces
- **Overly complex functions**: Break down large functions into smaller ones
- **Tight coupling**: Keep modules independent and testable

### 10. Performance Considerations
- **Lazy loading**: Load data and models only when needed
- **Batch processing**: Process data in batches for efficiency
- **Caching**: Cache expensive computations when appropriate
- **Profiling**: Use profilers to identify bottlenecks
- **Memory monitoring**: Track GPU and CPU memory usage

### 11. Additional Code Standards

#### Function Encapsulation and Documentation
- **Function size**: Keep functions focused and concise (max 50 lines per function)
- **Comprehensive docstrings**: Every function must have Google-style docstrings with:
  - Brief description of functionality
  - Args: parameter types, shapes (for tensors/arrays), and descriptions
  - Returns: return type, shape, and description
  - Raises: any exceptions that may be raised
  - Example: usage example when helpful
- **Type hints**: Use detailed type hints including tensor shapes when possible
  ```python
  def process_batch(data: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
      """Process a batch of data through the model.
      
      Args:
          data: Input tensor of shape [batch_size, channels, height, width]
          labels: Ground truth labels of shape [batch_size, num_classes]
          
      Returns:
          Processed output tensor of shape [batch_size, output_dim]
          
      Raises:
          ValueError: If input dimensions don't match expected shapes
      """
  ```

#### Configuration Management with Tyro
- **Prioritize tyro**: Use tyro for all new configuration management
- **Dataclass configs**: Define configurations as dataclasses with type hints
- **Hierarchical structure**: Organize configs into logical groups (training, data, model)
- **Default values**: Provide sensible defaults for all parameters
- **CLI generation**: Leverage tyro's automatic CLI generation

#### Language and Comments
- **English only**: All code, comments, docstrings, and variable names must be in English
- **No Chinese characters**: Remove any Chinese text from code and comments
- **Clear naming**: Use descriptive English names for variables and functions
- **Professional comments**: Write clear, professional English comments

#### Testing and Cleanup
- **Test after implementation**: Always test functionality after major changes
- **Clean up test files**: Remove temporary test files and test code after verification
- **Minimal test footprint**: Keep test code minimal and focused
- **Documentation of tests**: Briefly document what was tested in commit messages

#### Experiment Management and Results
- **Structured results directory**:
  ```
  results/
  ├── YYYY-MM-DD/
  │   └── exp_name_hash8/
  │       ├── config.yaml
  │       ├── checkpoints/
  │       ├── logs/
  │       ├── plots/
  │       └── inference_results/
  ```
- **Experiment naming**: `exp_name + first_8_chars_of_config_hash`
- **Required artifacts**: Each experiment must save:
  - `config.yaml`: Complete configuration used
  - Model checkpoints with epoch numbers
  - Loss curves and training plots
  - Inference results and metrics
  - Training and evaluation logs
- **Config file loading**: Support loading from config file path
  - If config_path is None: use default parameters
  - If config_path provided: load and override defaults
  - Always validate loaded configurations

#### Implementation Patterns
```python
# Good: Proper function with docstring and type hints
def calculate_loss(predictions: torch.Tensor, targets: torch.Tensor, 
                  reduction: str = 'mean') -> torch.Tensor:
    """Calculate MSE loss between predictions and targets.
    
    Args:
        predictions: Model predictions of shape [batch_size, output_dim]
        targets: Ground truth targets of shape [batch_size, output_dim]
        reduction: Reduction method ('mean', 'sum', 'none')
        
    Returns:
        Loss tensor, scalar if reduction != 'none', otherwise [batch_size]
        
    Raises:
        ValueError: If prediction and target shapes don't match
    """
    if predictions.shape != targets.shape:
        raise ValueError(f"Shape mismatch: {predictions.shape} vs {targets.shape}")
    
    loss = F.mse_loss(predictions, targets, reduction=reduction)
    return loss

# Good: Experiment setup with proper naming
def setup_experiment(config: TrainingConfig) -> Tuple[str, str]:
    """Set up experiment directory with proper naming convention.
    
    Args:
        config: Training configuration object
        
    Returns:
        Tuple of (experiment_directory, config_hash)
    """
    config_hash = get_config_hash(config)[:8]
    exp_dir = f"{config.results_path}/{config.date_exp}/{config.exp_name}_{config_hash}"
    os.makedirs(exp_dir, exist_ok=True)
    return exp_dir, config_hash
```

## Summary
These rules ensure code quality, maintainability, and reproducibility for research projects. Always prioritize clarity, documentation, and error handling. Use tyro for configuration management, maintain English-only codebase, test thoroughly, and follow structured experiment management. When in doubt, follow existing patterns in the codebase and academic best practices.
